# AI・エージェントアーキテクチャ (Workers AI)

本システムの核心である「AIチューター」機能は、Cloudflare Workers AI上で動作するLLMによって実現される。ここでは、単純なチャットボットではなく、学習状況を判断し、適切なガイダンスを行う「エージェント」としての振る舞いを実装する。

## モデル選定戦略

タスクの難易度とコストのバランスを考慮し、複数のモデルを使い分ける。

### Llama 3.1 8B Instruct (`@cf/meta/llama-3.1-8b-instruct`)

- **用途**: 通常の会話、ヒントの提示、コードの構文チェック
- **理由**: 非常に高速かつ低コストであり、日本語の流暢さも十分に高い

### Llama 3.3 70B (`@cf/meta/llama-3.3-70b-instruct-fp8-fast`)

- **用途**: クエストの最終評価、複雑な論理的誤りの指摘、シナリオ生成
- **理由**: GPT-4クラスの推論能力を持ちながら、Cloudflare上ではオープンモデルとして安価に利用可能

## Function Callingによるエージェント化

Llama 3.1はFunction Calling（ツール利用）をネイティブでサポートしている。これにより、AIは単にテキストを返すだけでなく、システム上の操作を実行できるようになる。

### 実装するツール（Functions）の例

| 関数名 | 説明 | パラメータ |
|-------|------|-----------|
| `submit_answer` | ユーザーの回答を提出し、クエスト完了判定を行う | `code: string`, `confidence: number` |
| `request_hint` | ヒントを表示する（XPを消費） | `level: 1-3` |
| `search_docs` | 関連する技術ドキュメント（RAG）を検索する | `query: string` |

### 処理フロー

1. ユーザーの入力を受け取る
2. Workerがシステムプロンプトに「利用可能なツール定義（JSONスキーマ）」を含めてLLMに送信
3. LLMが「ツールを実行すべき」と判断した場合、特定のJSONをレスポンスとして返す
4. WorkerがそのJSONを解析し、実際にロジック（例：D1の更新、検索）を実行
5. 実行結果を再度LLMに入力し、最終的な回答（自然言語）を生成させる

## ストリーミングレスポンスの実装

学習体験において「AIが考えて書いている」演出と、待機時間の短縮は重要である。Workers AIはレスポンスのストリーミングをサポートしているため、Honoの `streamText` ヘルパーを利用してクライアントに逐次データを送信する。

```typescript
import { streamText } from 'hono/streaming'

app.post('/chat', async (c) => {
  const { messages } = await c.req.json()
  
  return streamText(c, async (stream) => {
    const response = await c.env.AI.run('@cf/meta/llama-3.1-8b-instruct', {
      messages,
      stream: true // ストリーミング有効化
    })

    for await (const chunk of response) {
      if (chunk.response) {
        await stream.write(chunk.response)
      }
    }
  })
})
```
