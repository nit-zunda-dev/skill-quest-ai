# AI・エージェントアーキテクチャ (Workers AI)

本システムの核心である「AIチューター」機能は、Cloudflare Workers AI上で動作するLLMによって実現される。ここでは、単純なチャットボットではなく、学習状況を判断し、適切なガイダンスを行う「エージェント」としての振る舞いを実装する。

## モデル選定戦略

タスクの難易度とコストのバランスを考慮し、複数のモデルを使い分ける。

### Llama 3.1 8B Instruct (`@cf/meta/llama-3.1-8b-instruct`)

- **用途**: 通常の会話、ヒントの提示、コードの構文チェック
- **理由**: 非常に高速かつ低コストであり、日本語の流暢さも十分に高い

### Llama 3.3 70B (`@cf/meta/llama-3.3-70b-instruct-fp8-fast`)

- **用途**: クエストの最終評価、複雑な論理的誤りの指摘、シナリオ生成
- **理由**: GPT-4クラスの推論能力を持ちながら、Cloudflare上ではオープンモデルとして安価に利用可能

## Function Callingによるエージェント化

Llama 3.1はFunction Calling（ツール利用）をネイティブでサポートしている。これにより、AIは単にテキストを返すだけでなく、システム上の操作を実行できるようになる。

### 実装するツール（Functions）の例

| 関数名 | 説明 | パラメータ |
|-------|------|-----------|
| `submit_answer` | ユーザーの回答を提出し、クエスト完了判定を行う | `code: string`, `confidence: number` |
| `request_hint` | ヒントを表示する（XPを消費） | `level: 1-3` |
| `search_docs` | 関連する技術ドキュメント（RAG）を検索する | `query: string` |

### 処理フロー

1. ユーザーの入力を受け取る
2. Workerがシステムプロンプトに「利用可能なツール定義（JSONスキーマ）」を含めてLLMに送信
3. LLMが「ツールを実行すべき」と判断した場合、特定のJSONをレスポンスとして返す
4. WorkerがそのJSONを解析し、実際にロジック（例：D1の更新、検索）を実行
5. 実行結果を再度LLMに入力し、最終的な回答（自然言語）を生成させる

## ニューロン制約と対策

Workers AI Freeプランでは **1日あたり 10,000 Neurons** の上限がある（00:00 UTC でリセット）。1回のキャラ生成（Llama 3.1 8B）で約 19 Neurons 消費するため、無対策では約 500 回/日が上限となる。複数エンドポイント（ナラティブ、パートナーメッセージ、チャット）を合わせると早々に枯渇しうる。以下で Free 枠内での運用を可能にする。

### AI Gateway によるキャッシュ

[Cloudflare AI Gateway](https://developers.cloudflare.com/ai-gateway/) を経由して Workers AI を呼び出すと、**同一プロンプトの再リクエストはキャッシュヒット**し、ニューロン消費が発生しない。本システムでは `env.AI.run()` 呼び出しにオプションで `gateway: { id: "<Gateway ID>" }` を渡し、Gateway を有効化する。キャラ生成・ナラティブ・パートナーメッセージはいずれも「同じ入力なら同じ出力」でよいため、キャッシュの効果が大きい。

### レート制限

AI エンドポイント（`/api/ai/*`）に対して、ユーザー（または IP）ごとのレート制限を設ける（例: 1分あたり N 回まで）。乱用や連打によるニューロン枯渇を防ぐ。実装は Worker 内のメモリ/KV ベースのカウンタ、または Cloudflare Rate Limiting ルールを検討する。

### 日次閾値超過時のフォールバック

Free 枠（10,000/日）に近づいたら、Worker 側で「今日の使用量」を概算し、閾値（例: 9,000 Neurons）を超えた場合は **Workers AI を呼ばずスタブ応答**に切り替える。キャラ生成は `defaultCharacterProfile`、ナラティブ・パートナーメッセージ・チャットは固定文または「AI 利用一時制限中」を返す。これにより上限超過による 503 や意図しない課金を防ぎつつサービスは継続できる。使用量の概算は「1リクエストあたり平均 20 Neurons」などの仮定で D1 に日次集計する簡易方式でよい。

## ストリーミングレスポンスの実装

学習体験において「AIが考えて書いている」演出と、待機時間の短縮は重要である。Workers AIはレスポンスのストリーミングをサポートしているため、Honoの `streamText` ヘルパーを利用してクライアントに逐次データを送信する。

```typescript
import { streamText } from 'hono/streaming'

app.post('/chat', async (c) => {
  const { messages } = await c.req.json()
  
  return streamText(c, async (stream) => {
    const response = await c.env.AI.run('@cf/meta/llama-3.1-8b-instruct', {
      messages,
      stream: true // ストリーミング有効化
    })

    for await (const chunk of response) {
      if (chunk.response) {
        await stream.write(chunk.response)
      }
    }
  })
})
```
