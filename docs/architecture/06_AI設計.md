# AI・エージェントアーキテクチャ (Workers AI)

本システムの核心である「AIチューター」機能は、Cloudflare Workers AI上で動作するLLMによって実現される。ここでは、単純なチャットボットではなく、学習状況を判断し、適切なガイダンスを行う「エージェント」としての振る舞いを実装する。

## モデル選定戦略

タスクの難易度とコストのバランスを考慮し、複数のモデルを使い分ける。

### Llama 3.1 8B Instruct (`@cf/meta/llama-3.1-8b-instruct`)

- **用途**: 通常の会話、ヒントの提示、コードの構文チェック
- **理由**: 非常に高速かつ低コストであり、日本語の流暢さも十分に高い

### Llama 3.3 70B (`@cf/meta/llama-3.3-70b-instruct-fp8-fast`)

- **用途**: クエストの最終評価、複雑な論理的誤りの指摘、シナリオ生成
- **理由**: GPT-4クラスの推論能力を持ちながら、Cloudflare上ではオープンモデルとして安価に利用可能

## Function Callingによるエージェント化

Llama 3.1はFunction Calling（ツール利用）をネイティブでサポートしている。これにより、AIは単にテキストを返すだけでなく、システム上の操作を実行できるようになる。

### 実装するツール（Functions）の例

| 関数名 | 説明 | パラメータ |
|-------|------|-----------|
| `submit_answer` | ユーザーの回答を提出し、クエスト完了判定を行う | `code: string`, `confidence: number` |
| `request_hint` | ヒントを表示する（XPを消費） | `level: 1-3` |
| `search_docs` | 関連する技術ドキュメント（RAG）を検索する | `query: string` |

### 処理フロー

1. ユーザーの入力を受け取る
2. Workerがシステムプロンプトに「利用可能なツール定義（JSONスキーマ）」を含めてLLMに送信
3. LLMが「ツールを実行すべき」と判断した場合、特定のJSONをレスポンスとして返す
4. WorkerがそのJSONを解析し、実際にロジック（例：D1の更新、検索）を実行
5. 実行結果を再度LLMに入力し、最終的な回答（自然言語）を生成させる

## 利用制限ポリシー（AI戦略）

ニューロン消費を抑え、安価に運用するため、**エンドポイントごとに利用回数制限**を設ける。制限はアカウント単位で管理し、サブスクリプション（有料プラン）で上限を解放または緩和する。

| エンドポイント | 無料・通常 | サブスク時 | 備考 |
|----------------|------------|------------|------|
| **POST /api/ai/generate-character** | 新規アカウントにつき **1回限り** | 同左（変更なし） | キャラは1アカウント1体の前提 |
| **POST /api/ai/generate-narrative** | アカウントにつき **1日1回** | 上限解放 or 日次N回に増量 | タスク完了ごとのナラティブ・報酬 |
| **POST /api/ai/generate-partner-message** | アカウントにつき **1日1回** | 上限解放 or 日次N回に増量 | パートナーセリフ |
| **POST /api/ai/chat** | アカウントにつき **1日N回**（例: 10回） | 上限解放 or 日次回数増 | ストリーミングチャット |

### 設計の要点

- **キャラクター生成**: 初回のみ許可。既にキャラ生成済みかは D1 のユーザー/プロフィールテーブルで判定する。
- **日次制限**: ナラティブ・パートナー・チャットは「その日（UTC または JST）の利用回数」を D1 または KV でカウントし、上限に達したら **429 Too Many Requests** またはスタブ応答を返す。
- **サブスク**: アカウントに `subscription_tier`（free / premium 等）を持たせ、プレミアム時は日次上限を引き上げるか無制限にする。
- **未認証**: 認証必須にすれば「アカウント単位」の制限がかけやすい。未認証の場合は IP 単位のレート制限にフォールバックする運用も可。

実装はミドルウェアまたは各ルート内で「利用回数取得 → 上限チェック → 超過時は 429 またはスタブ返却」とする。キャラ生成（1回限り）に加え、ナラティブ・パートナー・チャット等の日次制限も **Backend** で実施する。

制限の**強制**は **Backend** で行う（各 AI エンドポイントのハンドラ内またはミドルウェアで、利用回数を取得・判定し、超過時は 429 またはスタブ応答を返す）。**Frontend** は、残り回数の表示および制限到達時のボタン無効化で UX を補助する。制限の信頼できる判定は Backend に依存する。

---

## ニューロン制約と対策

Workers AI Freeプランでは **1日あたり 10,000 Neurons** の上限がある（00:00 UTC でリセット）。1回のキャラ生成（Llama 3.1 8B）で約 19 Neurons 消費するため、無対策では約 500 回/日が上限となる。複数エンドポイント（ナラティブ、パートナーメッセージ、チャット）を合わせると早々に枯渇しうる。上記の**利用制限ポリシー**と以下で Free 枠内での運用を可能にする。

### レート制限

AI エンドポイント（`/api/ai/*`）には、上記**利用制限ポリシー**に加え、**短時間の連打防止**としてユーザー（または IP）ごとのレート制限を設ける（例: 1分あたり N 回まで）。乱用によるニューロン枯渇を防ぐ。実装は Worker 内のメモリ/KV ベースのカウンタ、または Cloudflare Rate Limiting ルールを検討する。

### 日次閾値超過時のフォールバック

Free 枠（10,000/日）に近づいたら、Worker 側で「今日の使用量」を概算し、閾値（例: 9,000 Neurons）を超えた場合は **Workers AI を呼ばずスタブ応答**に切り替える。キャラ生成は `defaultCharacterProfile`、ナラティブ・パートナーメッセージ・チャットは固定文または「AI 利用一時制限中」を返す。これにより上限超過による 503 や意図しない課金を防ぎつつサービスは継続できる。使用量の概算は「1リクエストあたり平均 20 Neurons」などの仮定で D1 に日次集計する簡易方式でよい。

### Neurons 概算係数と Cloudflare 課金単位の対応（Task 1.4）

Cloudflare Workers AI の課金単位は **Neurons**。API レスポンスに Neurons は含まれないため、本システムでは操作種別ごとの**係数**で概算し、D1 の `ai_daily_usage.neurons_estimate` に日次集計する。係数はバックエンドの定数（`apps/backend/src/services/ai-usage.ts`）で定義し、Cloudflare の料金表・実測（ダッシュボード）と照らして調整する。

| 操作種別 | エンドポイント／処理 | 係数 (Neurons 概算) | 備考 |
|----------|----------------------|----------------------|------|
| キャラ生成 | `POST /api/ai/generate-character` | 19 | Llama 3.1 8B 実測に基づく |
| ナラティブ | `POST /api/ai/generate-narrative` | 20 | 1 リクエストあたり平均 |
| パートナー | `POST /api/ai/generate-partner-message` | 20 | 同上 |
| チャット | `POST /api/ai/chat` | 20 | 同上 |
| グリモワール | グリモワール生成 | 20 | 同上 |
| 目標更新 | 目標更新 AI | 20 | 同上 |

- **参照**: [Workers AI Pricing](https://developers.cloudflare.com/workers-ai/platform/pricing/)（Cloudflare 公式の Neurons と料金）
- **実測**: Cloudflare ダッシュボードで日次・モデル別の Neurons 消費を確認し、係数がずれている場合は定数と本表を更新する。

## ストリーミングレスポンスの実装

学習体験において「AIが考えて書いている」演出と、待機時間の短縮は重要である。Workers AIはレスポンスのストリーミングをサポートしているため、Honoの `streamText` ヘルパーを利用してクライアントに逐次データを送信する。

```typescript
import { streamText } from 'hono/streaming'

app.post('/chat', async (c) => {
  const { messages } = await c.req.json()
  
  return streamText(c, async (stream) => {
    const response = await c.env.AI.run('@cf/meta/llama-3.1-8b-instruct', {
      messages,
      stream: true // ストリーミング有効化
    })

    for await (const chunk of response) {
      if (chunk.response) {
        await stream.write(chunk.response)
      }
    }
  })
})
```
